# dataset_config_validator.py
import importlib.util
import sys
from pathlib import Path
from typing import Dict, Any, List
import traceback

class OpenCompassConfigValidator:
    """é€šç”¨çš„OpenCompassé…ç½®éªŒè¯å™¨"""
    
    def __init__(self, config_file_path: str):
        """
        Args:
            config_file_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚ 'ARC_c_test_gen.py'
        """
        self.config_path = Path(config_file_path)
        self.config_module = self._load_config_module()
        
    def _load_config_module(self):
        """åŠ¨æ€åŠ è½½é…ç½®æ¨¡å—"""
        spec = importlib.util.spec_from_file_location("config", self.config_path)
        module = importlib.util.module_from_spec(spec)
        sys.modules["config"] = module
        spec.loader.exec_module(module)
        return module
    
    def validate_all_datasets(self, num_samples: int = 3) -> Dict[str, Any]:
        """éªŒè¯é…ç½®æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ•°æ®é›†é…ç½®"""
        # è‡ªåŠ¨æ‰¾åˆ°é…ç½®ä¸­çš„æ•°æ®é›†åˆ—è¡¨
        datasets_var = None
        for attr_name in dir(self.config_module):
            attr = getattr(self.config_module, attr_name)
            if isinstance(attr, list) and len(attr) > 0:
                if isinstance(attr[0], dict) and 'type' in attr[0]:
                    datasets_var = attr
                    print(f"ğŸ” æ‰¾åˆ°æ•°æ®é›†é…ç½®å˜é‡: {attr_name}")
                    break
        
        if datasets_var is None:
            return {"error": "æœªæ‰¾åˆ°æ•°æ®é›†é…ç½®åˆ—è¡¨"}
        
        results = {}
        for i, dataset_config in enumerate(datasets_var):
            dataset_name = dataset_config.get('abbr', f'dataset_{i}')
            print(f"\n{'='*50}")
            print(f"éªŒè¯æ•°æ®é›†: {dataset_name}")
            print(f"{'='*50}")
            
            result = self._validate_single_dataset(dataset_config, num_samples)
            results[dataset_name] = result
            
        return results
    
    def _validate_single_dataset(self, dataset_config: Dict, num_samples: int = 3) -> Dict[str, Any]:
        """éªŒè¯å•ä¸ªæ•°æ®é›†é…ç½®"""
        result = {
            'dataset_loaded': False,
            'columns_match': False,
            'prompt_generated': False,
            'samples': [],
            'generated_prompts': [],
            'errors': []
        }
        
        try:
            # 1. åŠ è½½æ•°æ®é›†
            dataset_class = dataset_config['type']
            dataset_kwargs = {k: v for k, v in dataset_config.items() 
                            if k not in ['type', 'reader_cfg', 'infer_cfg', 'eval_cfg', 'abbr']}
            
            dataset = dataset_class(**dataset_kwargs)
            result['dataset_loaded'] = True
            print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
            
            if len(dataset) == 0:
                result['errors'].append("æ•°æ®é›†ä¸ºç©º")
                return result
                
            # 2. æ£€æŸ¥æ•°æ®ç»“æ„
            sample = dataset[0]
            actual_columns = list(sample.keys())
            print(f"ğŸ“‹ å®é™…æ•°æ®åˆ—: {actual_columns}")
            
            reader_cfg = dataset_config.get('reader_cfg', {})
            if 'input_columns' in reader_cfg:
                input_columns = reader_cfg['input_columns']
                missing_cols = [col for col in input_columns if col not in actual_columns]
                if missing_cols:
                    result['errors'].append(f"ç¼ºå°‘è¾“å…¥åˆ—: {missing_cols}")
                    print(f"âŒ ç¼ºå°‘è¾“å…¥åˆ—: {missing_cols}")
                else:
                    result['columns_match'] = True
                    print(f"âœ… æ‰€æœ‰è¾“å…¥åˆ—éƒ½å­˜åœ¨: {input_columns}")
            
            if 'output_column' in reader_cfg:
                output_col = reader_cfg['output_column']
                if output_col not in actual_columns:
                    result['errors'].append(f"ç¼ºå°‘è¾“å‡ºåˆ—: {output_col}")
                    print(f"âŒ ç¼ºå°‘è¾“å‡ºåˆ—: {output_col}")
                else:
                    print(f"âœ… è¾“å‡ºåˆ—å­˜åœ¨: {output_col}")
            
            # 3. æ”¶é›†æ ·æœ¬æ•°æ®
            for i in range(min(num_samples, len(dataset))):
                sample = dataset[i]
                result['samples'].append(dict(sample))
                
            # 4. æµ‹è¯•promptç”Ÿæˆ
            infer_cfg = dataset_config.get('infer_cfg', {})
            if 'prompt_template' in infer_cfg:
                self._test_prompt_generation(dataset, infer_cfg, result, num_samples)
            
            # 5. æµ‹è¯•åå¤„ç†å™¨
            eval_cfg = dataset_config.get('eval_cfg', {})
            if 'pred_postprocessor' in eval_cfg:
                self._test_postprocessor(eval_cfg, result)
                
        except Exception as e:
            result['errors'].append(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
            print(f"âŒ éªŒè¯å¤±è´¥: {e}")
            print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
        return result
    
    def _test_prompt_generation(self, dataset, infer_cfg, result, num_samples):
        """æµ‹è¯•promptç”Ÿæˆ"""
        try:
            from opencompass.openicl.icl_prompt_template import PromptTemplate
            
            prompt_template_cfg = infer_cfg['prompt_template']
            if isinstance(prompt_template_cfg, dict) and prompt_template_cfg.get('type') == PromptTemplate:
                template = prompt_template_cfg['template']
            else:
                template = prompt_template_cfg
                
            prompt_template = PromptTemplate(template=template)
            
            print(f"\nğŸ¯ æµ‹è¯•Promptç”Ÿæˆ:")
            for i in range(min(num_samples, len(dataset))):
                sample = dataset[i]
                try:
                    generated_prompt = prompt_template.generate_prompt_for_generate_task(sample)
                    result['generated_prompts'].append({
                        'sample_index': i,
                        'prompt': generated_prompt,
                        'sample_data': dict(sample)
                    })
                    print(f"\n--- æ ·æœ¬ {i+1} çš„Prompt ---")
                    print(generated_prompt[:500] + "..." if len(generated_prompt) > 500 else generated_prompt)
                    
                except Exception as e:
                    result['errors'].append(f"æ ·æœ¬{i}ç”Ÿæˆpromptå¤±è´¥: {str(e)}")
                    print(f"âŒ æ ·æœ¬{i}ç”Ÿæˆpromptå¤±è´¥: {e}")
                    
            result['prompt_generated'] = len(result['generated_prompts']) > 0
            if result['prompt_generated']:
                print(f"âœ… æˆåŠŸç”Ÿæˆ {len(result['generated_prompts'])} ä¸ªprompt")
                
        except Exception as e:
            result['errors'].append(f"Promptç”Ÿæˆæµ‹è¯•å¤±è´¥: {str(e)}")
            print(f"âŒ Promptæµ‹è¯•å¤±è´¥: {e}")
    
    def _test_postprocessor(self, eval_cfg, result):
        """æµ‹è¯•åå¤„ç†å™¨"""
        try:
            postprocessor_cfg = eval_cfg.get('pred_postprocessor', {})
            if not postprocessor_cfg:
                return
                
            print(f"\nğŸ”§ æµ‹è¯•åå¤„ç†å™¨:")
            
            # åŠ¨æ€å¯¼å…¥åå¤„ç†å‡½æ•°
            if 'type' in postprocessor_cfg:
                func = postprocessor_cfg['type']
                kwargs = {k: v for k, v in postprocessor_cfg.items() if k != 'type'}
                
                test_responses = ["A", "The answer is B", "C.", "I think the answer is D", "é€‰æ‹©A"]
                for resp in test_responses:
                    try:
                        processed = func(resp, **kwargs)
                        print(f"  '{resp}' -> '{processed}'")
                    except Exception as e:
                        print(f"  '{resp}' -> å¤„ç†å¤±è´¥: {e}")
                        
        except Exception as e:
            result['errors'].append(f"åå¤„ç†å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
            print(f"âŒ åå¤„ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
    
    def generate_report(self, results: Dict[str, Any]) -> str:
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        report = ["\n" + "="*60]
        report.append("OpenCompassé…ç½®éªŒè¯æŠ¥å‘Š")
        report.append("="*60)
        
        for dataset_name, result in results.items():
            if 'error' in result:
                report.append(f"\nâŒ {dataset_name}: {result['error']}")
                continue
                
            report.append(f"\nğŸ“Š æ•°æ®é›†: {dataset_name}")
            report.append(f"  æ•°æ®é›†åŠ è½½: {'âœ…' if result['dataset_loaded'] else 'âŒ'}")
            report.append(f"  åˆ—ååŒ¹é…: {'âœ…' if result['columns_match'] else 'âŒ'}")
            report.append(f"  Promptç”Ÿæˆ: {'âœ…' if result['prompt_generated'] else 'âŒ'}")
            report.append(f"  æ ·æœ¬æ•°é‡: {len(result['samples'])}")
            report.append(f"  ç”Ÿæˆçš„Promptæ•°: {len(result['generated_prompts'])}")
            
            if result['errors']:
                report.append("  é”™è¯¯ä¿¡æ¯:")
                for error in result['errors']:
                    report.append(f"    - {error}")
                    
        return "\n".join(report)

def validate_config(config_file: str, num_samples: int = 2):
    """éªŒè¯é…ç½®æ–‡ä»¶çš„ä¾¿æ·å‡½æ•°"""
    validator = OpenCompassConfigValidator(config_file)
    results = validator.validate_all_datasets(num_samples)
    report = validator.generate_report(results)
    print(report)
    return results

if __name__ == "__main__":
    import sys
    if len(sys.argv) > 1:
        config_file = sys.argv[1]
        num_samples = int(sys.argv[2]) if len(sys.argv) > 2 else 2
        validate_config(config_file, num_samples)
    else:
        print("ç”¨æ³•: python config_validator.py <é…ç½®æ–‡ä»¶è·¯å¾„> [æ ·æœ¬æ•°é‡]")
